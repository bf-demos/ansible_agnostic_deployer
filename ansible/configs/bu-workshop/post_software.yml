# vim: set ft=ansible:
---
- name: Bastion hostname config
  hosts: localhost
  connection: local
  become: false
  tags:
    - workshop
    - workshop_bastion_hostname
  tasks:
    - name: Store bastion hostname as a fact
      set_fact:
        bastion_hostname: "{{ hostvars[ groups[ ('tag_' ~ env_type ~ '_' ~ guid ~ '_bastion') | replace('-', '_') ].0 ]['ec2_public_dns_name'] }}"

# TODO: metrics nodeselector

- name: User volumes on NFS server
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') }}"
  gather_facts: False
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_user_nfs_volume_dirs
  tasks:
    - name: Create user vols
      shell: "mkdir -p /srv/nfs/user-vols/vol{1..{{ user_vols }}}"
    - name: chmod the user vols
      shell: "chmod -R 777 /srv/nfs/user-vols"

- name: NFS volume configuration
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_nfs_user_base_pv
  tasks:
    - name: Set NFS related facts
      set_fact:
        nfs_host: "{{ hostvars[ groups[ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') ].0 ]['ec2_public_dns_name'] }}"
        pv_size: '10Gi'
        pv_list: "{{ nfs_shares }}"
        persistentVolumeReclaimPolicy: Retain

    # TODO: I think I can get rid of this pvs because I do it later
    - name: Generate PV file
      template:
        src: "{{ playbook_dir }}/files/pvs.j2"
        dest: "/root/pvs-{{ env_type }}-{{ guid }}.yml"
      tags: 
        - gen_pv_file

    - set_fact:
        pv_size: "{{ user_vols_size }}"
        persistentVolumeReclaimPolicy: Recycle

    - name: Generate user vol PV file
      template:
        src: "{{ playbook_dir }}/files/userpvs.j2"
        dest: "/root/userpvs-{{ env_type }}-{{ guid }}.yml"
      tags:
        - gen_user_vol_pv

    - name: Create base PVs from file
      shell: '{{ oc_path }} create -f /root/pvs-{{ env_type }}-{{ guid }}.yml || {{ oc_path }} replace -f /root/pvs-{{ env_type }}-{{ guid }}.yml'

    - name: Create user PVs from file
      shell: '{{ oc_path }} create -f /root/userpvs-{{ env_type }}-{{ guid }}.yml || {{ oc_path }} replace -f /root/userpvs-{{ env_type }}-{{ guid }}.yml'

- name: Gluster CNS clients and topology
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_cns_client
    - cns_tasks
  tasks:
    - name: Install Gluster client software
      yum:
        name: '{{ item }}'
        state: present
        enablerepo: "rh-gluster-3-for-rhel-7-server-rpms"
      with_items:
        - "cns-deploy"
        - "heketi-client"

    - name: Store infra node info as facts
      set_fact:
        private_dns_name: "{{ hostvars[ groups[ ('tag_' ~ env_type ~ '_' ~ guid ~ '_infranode') | replace('-', '_') ].0 ]['ec2_private_dns_name'] }}"
        private_ip_address: "{{ hostvars[ groups[ ('tag_' ~ env_type ~ '_' ~ guid ~ '_infranode') | replace('-', '_') ].0 ]['ec2_private_ip_address'] }}"

    - name: Label the infra node for gluster
      command: "{{ oc_path }} label node {{ private_dns_name }} storagenode=glusterfs --overwrite"

    - name: Generate heketi topology JSON
      template:
        src: "{{ playbook_dir }}/files/heketi-topology.json.j2"
        dest: "/root/heketi-topology.json"

    - name: Check for storage project
      command: "{{ oc_path }} get project storage"
      register: storage_project
      ignore_errors: true

    - name: Create storage project
      command: "{{ oc_path }} adm new-project storage --admin admin --node-selector='env=infra'" 
      when: storage_project | failed

    - name: Set storage project policy
      command: "{{ oc_path }} adm policy add-scc-to-user privileged system:serviceaccount:storage:default"

- name: Gluster CNS nodes
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_infranode') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_cns_nodes
    - cns_tasks
  tasks:
    - name: Insert firewall rule for CNS
      lineinfile:
        dest: "/etc/sysconfig/iptables"
        line: "{{ item }}"
        insertbefore: "COMMIT"
      register: cns_rules
      with_items:
        - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT"
        - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT"
        - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT"
        - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT"

    - name: Reload firewall config
      service:
        name: "iptables"
        state: reloaded
      when: cns_rules | changed

    - name: Create /var/lib/heketi_server correctly
      file:
        dest: "/var/lib/heketi_server"
        group: "root"
        mode: "0775"
        owner: "root"
        setype: "svirt_sandbox_file_t"
        state: directory
        recurse: yes

- name: Gluster CNS install
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  vars:
    - heketi_cli: "/bin/heketi-cli -s http://deploy-heketi.storage.svc.cluster.local:8080"
  tags:
    - workshop
    - workshop_cns_install
    - cns_tasks
  tasks:
    - name: Copy modified heketi template
      copy:
        src: "{{ playbook_dir }}/files/deploy-heketi-template.yaml"
        dest: "/root/deploy-heketi-template.yaml"

    - name: Create heketi templates in storage project
      shell: "{{ oc_path }} create -f {{ item }} -n storage || {{ oc_path }} replace -f {{ item }}"
      with_items:
        - "/root/deploy-heketi-template.yaml"
        - "/usr/share/heketi/templates/heketi-service-account.yaml"
        - "/usr/share/heketi/templates/glusterfs-template.yaml"

    - name: Give Heketi SA edit access
      command: "{{ oc_path }} adm policy add-role-to-user edit system:serviceaccount:storage:heketi-service-account"

    - name: Check for glusterfs pod
      shell: "{{ oc_path }} get pod --no-headers | grep glusterfs"
      register: glusterfs_exists
      ignore_errors: true

    - name: Process glusterfs pod template
      shell: "{{ oc_path }} process glusterfs -n storage | {{ oc_path }} create -f -"
      when: glusterfs_exists | failed

    - name: Wait for glusterfs pod to be running
      shell: "{{ oc_path }} get pod `{{ oc_path }} get pod --no-headers | awk '/glusterfs/ {print $1}'` -o yaml"
      register: glusterfs_pod
      until: '"Running" in glusterfs_pod.stdout'
      retries: 5
      delay: 60

    - name: Obtain the heketi mountable secret name
      shell: "{{ oc_path }} get sa heketi-service-account -o yaml | grep token | cut -d ' ' -f 3"
      register: heketi_secret

    - name: Check if deploy-heketi service exists
      shell: "{{ oc_path }} get svc deploy-heketi -n storage"
      register: heketi_service
      ignore_errors: true

    - name: Check if deploy-heketi pod exists
      shell: "{{ oc_path }} get pod -n storage | grep deploy-heketi"
      register: heketi_pod
      ignore_errors: true

    - name: Process deploy-heketi template
      shell: >
        {{ oc_path }} process deploy-heketi 
        -v HEKETI_KUBE_SECRETNAME={{ heketi_secret.stdout }}
        -v HEKETI_KUBE_INSECURE=y
        -v HEKETI_KUBE_NAMESPACE=storage
        -v HEKETI_KUBE_APIHOST=https://kubernetes.default.svc.cluster.local
        -n storage | {{ oc_path }} create -f -
      when: (heketi_service | failed) and (heketi_pod | failed)

    - name: Wait for deploy-heketi pod to be running
      shell: "{{ oc_path }} get pod `{{ oc_path }} get pod --no-headers | awk '/deploy-heketi/ {print $1}'` -o yaml"
      register: deploy_heketi_pod
      until: '"Running" in deploy_heketi_pod.stdout'
      retries: 5
      delay: 60

    - name: Execute heketi-cli and create topology
      shell: "{{ heketi_cli }} topology load --json=/root/heketi-topology.json"

    - name: Copy gluster storageclass template to master
      template:
        src: "{{ playbook_dir }}/files/gluster-storageclass.yaml.j2"
        dest: "/root/gluster-storageclass.yaml"

    - name: Create gluster storageclass definition
      shell: "{{ oc_path }} create -f /root/gluster-storageclass.yaml || {{ oc_path }} replace -f /root/gluster-storageclass.yaml"

    - name: Test create volume
      shell: "{{ heketi_cli }} volume create --size=1 --durability=none --json"
      register: volume_create

    - name: Delete created volume
      shell: "{{ heketi_cli }} volume delete {{ (volume_create.stdout|from_json) | json_query('id') }}"

- name: Workshop admins
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_admins
  tasks:
    - name: Add administrative user to htpasswd file
      htpasswd:
        dest: "/etc/origin/master/htpasswd"
        name: "admin"
        password: "openshift3"
        state: present

    - name: Give administrative user cluster-admin privileges
      command: "{{ oc_path }} adm policy add-cluster-role-to-user cluster-admin admin"

- name: Create Workshop NFS shares
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_nfs
  tasks:
    - name: Create workshop nfs directory
      file:
        name: '/srv/nfs/{{ item }}'
        state: directory
        mode: 0777
        owner: nfsnobody
        group: nfsnobody
        recurse: True
      with_items:
        - '{{ workshop_shares }}'

    - name: Create workshop exports file
      file:
        path: "/etc/exports.d/{{ env_type }}-{{ guid }}-workshop.exports"
        state: touch
        mode: 755

    - name: Update workshop exports file
      lineinfile:
        dest: "/etc/exports.d/{{ env_type }}-{{ guid }}-workshop.exports"
        line: "/srv/nfs/{{ item }} *(rw,root_squash,no_wdelay,sync)"
        state: present
      with_items:
        - '{{ workshop_shares }}'
      run_once: True

    - name: Reload NFS exports
      shell: "exportfs -r"

- name: Workshop PVs
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - create_workshop_vol_pvs
  tasks:
    - set_fact:
        pv_size: '10Gi'
        pv_list: "{{ workshop_shares }}"
        persistentVolumeReclaimPolicy: Retain
        nfs_hostname: "{{ hostvars[ groups[ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') ].0 ]['ansible_fqdn'] }}"

    - name: Generate workshop PV file
      template:
        src: "files/{{ env_type }}_pvs.j2"
        dest: "/root/pvs-{{ env_type }}-{{ guid }}.yml"

    - name: Create workshop PVs
      shell: '{{ oc_path }} create -f /root/pvs-{{ env_type }}-{{ guid }}.yml || {{ oc_path }} replace -f /root/pvs-{{ env_type }}-{{ guid }}.yml'

- name: Workshop prerequisites
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_prereq
  tasks:
    - name: Check for workshop-infra project
      command: "{{ oc_path }}  get project workshop-infra"
      register: result
      ignore_errors: true

    - name: Create workshop-infra project
      command: "{{ oc_path }} adm new-project workshop-infra --admin admin --node-selector='env=infra'"
      when: result | failed

    - name: Make workshop-infra project network global
      command: "{{ oc_path }} adm pod-network make-projects-global workshop-infra"

    - name: Set workshop-infra SCC for anyuid
      command: "{{ oc_path }} adm policy add-scc-to-group anyuid system:serviceaccounts:workshop-infra"

    - name: Add capabilities within anyuid which is not really ideal
      command: "{{ oc_path }} patch scc/anyuid --patch '{\"requiredDropCapabilities\":[\"MKNOD\",\"SYS_CHROOT\"]}'"

    #TODO: Switch to official Java builder
    - name: Copy RH official JDK IS to server
      copy:
        src: "files/openjdk-is.yaml"
        dest: "/root/openjdk-is.yaml"

    - name: Create RH official JDK IS in openshift namespace
      shell: "{{ oc_path }} create -f /root/openjdk-is.yaml -n openshift || {{ oc_path }} replace -f /root/openjdk-is.yaml -n openshift"

    - name: Copy simple-java-s2i IS to server
      copy:
        src: "files/java-s2i-is.yaml"
        dest: "/root/java-s2i-is.yaml"
      tags:
        - copy-java-s2i-is

    - name: Create simple-java-s2i IS in openshift namespace
      shell: "{{ oc_path }} create -f /root/java-s2i-is.yaml -n openshift || {{ oc_path }} replace -f /root/java-s2i-is.yaml -n openshift"
      tags:
        - create-java-s2i-is

    - name: Create Jenkins pipeline template in openshift namespace
      shell: "{{ oc_path }} create -f https://raw.githubusercontent.com/openshift-roadshow/nationalparks/1.0.0/ose3/pipeline-template.yaml -n openshift || {{ oc_path }} replace -f https://raw.githubusercontent.com/openshift-roadshow/nationalparks/1.0.0/ose3/pipeline-template.yaml -n openshift"
      tags:
        - create-pipeline-template

- name: Nexus server
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_nexus
  tasks:
    - name: Copy nexus.yaml to master
      copy:
        src: "files/nexus.yaml"
        dest: "/root/nexus.yaml"

    - name: Check if Nexus was already provisioned
      command: "{{ oc_path }} get service nexus -n workshop-infra"
      register: install_nexus
      ignore_errors: true

    - name: Instantiate nexus from template
      command: "{{ oc_path }} create -f /root/nexus.yaml -n workshop-infra"
      when: install_nexus | failed

    # looks like we need a better check - it seems we're ready up to several
    # seconds before the router finds out about us, so we might want another
    # http check to make sure nexus is responding
    - name: Wait for Nexus to be running
      command: "{{ oc_path }} get dc/nexus -o yaml -n workshop-infra"
      register: result
      until: '"availableReplicas: 1" in result.stdout'
      retries: 5
      delay: 60

    - name: Wait for Nexus to be happy
      uri:
        url: "http://nexus.workshop-infra.svc.cluster.local:8081/content/repositories/"
        status_code: 200
      register: nexus_happy
      until: nexus_happy | success
      retries: 5
      delay: 60

    - name: Install EPEL (for jq)
      package:
        name: "https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm"
        state: installed

    - name: Disable EPEL
      command: "yum-config-manager --disablerepo=epel"

    - name: Install jq
      package:
        name: jq
        state: present
        enablerepo: epel

    - name: Copy Nexus addrepo script
      copy:
        src: "files/addrepo.sh"
        dest: "/root/addrepo.sh"

    - name: Check for redhat-ga repository in Nexus
      uri:
        url: "http://nexus.workshop-infra.svc.cluster.local:8081/content/repositories/redhat-ga"
        status_code: 200
      register: redhat_ga_out
      ignore_errors: true

    - name: Add redhat-ga repository for Nexus
      shell: "NEXUS_BASE_URL=nexus.workshop-infra.svc.cluster.local:8081 bash /root/addrepo.sh redhat-ga https://maven.repository.redhat.com/ga/"
      when: redhat_ga_out | failed

    - name: Check for JBoss repository in Nexus
      uri:
        url: "http://nexus.workshop-infra.svc.cluster.local:8081/content/repositories/jboss"
        status_code: 200
      register: redhat_ga_out
      ignore_errors: true

    - name: Add redhat-ga repository for Nexus
      shell: "NEXUS_BASE_URL=nexus.workshop-infra.svc.cluster.local:8081 bash /root/addrepo.sh jboss https://repository.jboss.org/nexus/content/repositories/public"
      when: redhat_ga_out | failed

- name: Gitlab
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_gitlab
  tasks:
    - name: Copy gitlab-template.yaml to master
      copy:
        src: "files/gitlab-template.yaml"
        dest: "/root/gitlab-template.yaml"

    - name: Check if Gitlab was already provisioned
      command: "{{ oc_path }} get service gitlab-ce -n workshop-infra"
      register: install_gitlab
      ignore_errors: true

    - name: Instantiate Gitlab from template
      shell: >
        {{ oc_path }} process -f /root/gitlab-template.yaml
        -v APPLICATION_HOSTNAME=gitlab-ce-workshop-infra.{{ cloudapps_suffix }}
        -v GITLAB_ROOT_PASSWORD=password | {{ oc_path }} create -f - -n workshop-infra
      when: install_gitlab | failed
      tags:
        - instantiate-gitlab

    - name: Scale gitlab to four instances
      command: "{{ oc_path }} scale dc/gitlab-ce --replicas=4 -n workshop-infra"

    - name: Wait for Gitlab to be running
      command: "{{ oc_path }} get dc/gitlab-ce -o yaml -n workshop-infra"
      register: result
      until: '" availableReplicas: 4" in result.stdout'
      retries: 8
      delay: 60
      tags:
        - wait-for-gitlab

    - name: Annotate gitlab service to group database
      shell: >
        {{ oc_path }} annotate service gitlab-ce
        service.alpha.openshift.io/dependencies='[{"name":"gitlab-ce-postgresql","namespace":"","kind":"Service"},{"name":"gitlab-ce-redis","namespace":"","kind":"Service"}]' 
        --overwrite
        -n workshop-infra

- name: Lab guide
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_lab_guide
  tasks:
    - name: Check for workshop lab build
      command: "{{ oc_path }} get svc/labs -n workshop-infra"
      ignore_errors: true
      register: labs_service_out

    - name: Build workshop lab server
      shell: >
        {{ oc_path }} new-app
        --name=labs jboss-eap70-openshift~{{ lab_url }}#{{ lab_tag }}
        -e ROUTER_ADDRESS={{cloudapps_suffix}}
        -e CONSOLE_ADDRESS=master.{{subdomain_base}}
        -e DEFAULT_LAB=roadshow
        -e NUM_NODES={{ num_nodes }}
        -e NUM_USERS={{ user_vols }}
        -e WORKSHOPS_URLS={{ lab_content }}
        -n workshop-infra;
        {{ oc_path }} expose service labs -n workshop-infra
      when: labs_service_out | failed
      tags:
        - build-workshop-labs

- name: GitLab nfs permissions hack
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_nfs') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_nfs_hack
  tasks:
    - name: Fix ownership and permission of git-data
      file:
        dest: "/srv/nfs/gitlab-data"
        mode: "0700"
        owner: "998"
        group: root
        recurse: yes

    - name: Fix permission on git-data/repositories
      file:
        dest: "/srv/nfs/gitlab-data/git-data/repositories"
        mode: "2770"
        recurse: yes

- name: Project Request Template
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - project_request
  tasks:

    - name: Copy project request template to master
      copy:
        src: files/project-template.yaml
        dest: /root/project-template.yaml

    - name: Create project request template in default project
      shell: "{{ oc_path }} create -f /root/project-template.yaml -n default || {{ oc_path }} replace -f /root/project-template.yaml -n default"

- name: Workshop Users
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_users
  tasks:

    - name: Add log path to Ansible configuration
      lineinfile:
        regexp: "^#log_path"
        dest: "/etc/ansible/ansible.cfg"
        line: "log_path = /root/ansible.log"
        state: present

    - name: Copy vars file to master
      copy:
        src: "env_vars.yml"
        dest: "/root/{{ env_type }}_vars.yml"

    - name: Copy user provision Ansible script remotely
      copy:
        src: "files/userprovision.yaml"
        dest: "/root/userprovision.yaml"

    - name: Set Gitlab internal hostname
      set_fact:
        gitlab_hostname: 'gitlab-ce.workshop-infra.svc.cluster.local'

    - name: Get root user token
      uri:
        url: 'http://gitlab-ce.workshop-infra.svc.cluster.local/api/v3/session'
        body: 'login=root&password=password'
        method: POST
        status_code: 201
      register: root_token_out
      until: root_token_out|success
      retries: 3
      delay: 60

    - name: Create root token fact
      set_fact:
        root_token: '{{ root_token_out.json.private_token }}'

    - name: Execute user provision Ansible script remotely
      shell: >
        ansible-playbook
        -i localhost /root/userprovision.yaml
        -e config={{ env_type }}
        -e user={{ item }}
        -e root_token={{ root_token }}
        -e gitlab_hostname={{ gitlab_hostname }}
      with_sequence: start=0 end={{ user_vols }} format=%02d

- name: Deploy etherpad
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  tags:
    - workshop
    - workshop_etherpad
  tasks:
    - name: Check if MySQL for etherpad was already provisioned
      command: "{{ oc_path }} get service mysql-pad -n workshop-infra"
      register: install_pad_db
      ignore_errors: true

    - name: Etherpad MySQL
      shell: >
        {{ oc_path }} new-app 
        --name=mysql-pad
        --template=mysql-ephemeral
        -p MYSQL_USER=etherpad
        -p MYSQL_PASSWORD=etherpad
        -p MYSQL_DATABASE=etherpad
        -p DATABASE_SERVICE_NAME=mysql-pad
        -n workshop-infra
      when: install_pad_db | failed

    - name: Wait for Etherpad MySQL to be running
      command: "{{ oc_path }} get dc/mysql-pad -o yaml -n workshop-infra"
      register: result
      until: '"availableReplicas: 1" in result.stdout'
      retries: 5
      delay: 60

    - name: Check if Etherpad was already provisioned
      command: "{{ oc_path }} get service etherpad -n workshop-infra"
      register: install_pad
      ignore_errors: true

    # TODO: etherpad template or health check or something
    - name: Deploy Etherpad Docker image
      shell: >
        {{ oc_path }} new-app
        --name=etherpad
        centos/etherpad
        -e DB_HOST=mysql-pad
        -e DB_DBID=etherpad
        -e DB_PASS=etherpad
        -e DB_USER=etherpad
        -e DB_PORT=3306
        -n workshop-infra
      when: install_pad | failed

    - name: Wait for Etherpad to be running
      command: "{{ oc_path }} get dc/etherpad -o yaml -n workshop-infra"
      register: result
      until: '"availableReplicas: 1" in result.stdout'
      retries: 5
      delay: 60

    - name: Expose etherpad service
      shell: "{{ oc_path }} expose service etherpad -n workshop-infra"
      when: install_pad | failed

    - name: Annotate etherpad service to group database
      shell: >
        {{ oc_path }} annotate service etherpad 
        service.alpha.openshift.io/dependencies='[{"name":"mysql-pad","namespace":"","kind":"Service"}]' 
        --overwrite
        -n workshop-infra

    # TODO: curl command to create default pad

- name: Cache Java dependencies
  hosts: 
    - "{{ ('tag_' ~ env_type ~ '_' ~ guid ~ '_master') | replace('-', '_') }}"
  vars_files:
    - "env_vars.yml"
    - "env_secret_vars.yml"
    - "ssh_vars.yml"
  vars:
    workshop_repos:
      - "nationalparks"
      - "mlbparks"
      - "parksmap-web"
  tags:
    - workshop
    - workshop_java_dependencies
  tasks:
    - name: Install Maven and Java
      yum:
        name: '{{ item }}'
        state: present
        enablerepo: "rhel-7-server-optional-rpms"
      with_items:
        - "maven"
        - "java-1.8.0-openjdk-devel"

    - name: Remove m2 folder
      file:
        path: "/home/ec2-user/.m2/repository"
        state: absent

    - name: Make repos directory
      file:
        path: "/home/ec2-user/repos"
        state: directory

    - name: Clone app repositories
      git:
        repo: 'https://github.com/openshift-roadshow/{{ item }}'
        dest: "/home/ec2-user/repos/{{ item }}"
      with_items:
        - '{{ workshop_repos }}'

    - name: Deploy maven settings file
      template:
        src: "files/maven.xml.j2"
        dest: "/home/ec2-user/maven.xml"
        mode: 0755
        owner: ec2-user

    - name: Build and cache dependencies
      shell: >
        mvn -q -s /home/ec2-user/maven.xml -f /home/ec2-user/repos/{{ item }}/pom.xml install
      with_items:
        - '{{ workshop_repos }}'

